@file_name: [25_05_08]SearchEngine_vs_Blogger
@title: [25_05_08]検索エンジン vs ブロガー：槍と盾の戦いは現在進行形（長文注意！）
@date: 20250508
@tags: 技術

皆さんも過去のネットで検索ワードと全然関係ない結果で不快だった経験、きっとあると思います。

例えば、「ケーキのレシピ」を入力したとき、本文に文脈と外れても「ケーキのレシピ」単語そのものが沢山含まれると

検索結果の上位に表れる現象が確かにありました。多分２０００年度の中では普通でした。

この時期は、韓国だったらNAVER、日本だったらYahooが各国の検索エンジンを独占していました。

結果に何の連関性も無いページが出る現象は当時の全検索エンジンの共通点でした。

しかし２０１０年の序盤、Googleは優れる検索エンジンとして有名になり、全世界におけて有名になりました。

韓国の場合この時期Naverの占有率が下がり、Googleがその分人気を得るようになりました。

当時、何が起こったんでしょうか。






<h2>検索エンジンの基本的な流れ</h2>

検索エンジンは基本的に下の流れを従います。

1. Botでネットのドキュメントを収集
2. インデックス：収集したドキュメントから核心の単語のリストを抽出
3. 逆インデックス：抽出された単語ごとに、それを含むドキュメントのリストを作成する
4. ユーザーの検索キーワードに当たるドキュメントを表示する

例えば、あるブログの投稿に「ケーキを作る方法は～」という内容があると、検索エンジンは

「ケーキ」「作る」「方法」などの意味を持つ単語を抽出します。

そして「ケーキ」を検索したら表示するリストにそのブログの投稿を入れます。

すると、ユーザーが「ケーキの作り方」を入力した時、

検索キーワードは「ケーキ」「作る」「方法」に分かれます。

この分かれたキーワードに全部載っているブログの投稿は検索内容と合致、結果に表示されます。

ですが、これには欠点があります。皆さんも予想されるでしょう。

キーワードを単純にいっぱい入れ込めば入れ込むほど検索結果の上位化されることです。






<h2>２０１０年度序盤Googleの反撃</h2>

Googleはこの問題点を認識し、いくつのアルゴリズムを導入しました。

体表的な例：

- 特定キーワードを使いすぎると減点
- ユーザーのページ接続時間を測定、ページからすぐ離脱すると減点
- ページ離脱の後、すぐ他のページに接続するか測定、ユーザーが以前のページの情報に満足したか判定
- ページの中でスクロールやクリックなどのインタラクションを測定、
  他の情報と合わせて行動シグナルとして判定

このようなアルゴリズムを導入した後、上のキーワードのスパムの欠点は無くなりました。

結果、良い検索エンジンとして名を連ね始めました。

ブロガーの槍はGoogleという盾によって阻止されましたが、戦いはまだ終わっていません。






<h2>２０２３年、スパムブロガーの逆襲</h2>

２０２３年は生成AIの登場と普及で、ChatGPTやHuggingFaceを活用した自動生成ブログが量産されました。

一人で数時間に数百個の投稿を載せたり、本当にありそうな文章でブログに投稿をするが、

中身は空っぽの情報に出来ていたりするスパムブログは新しい形として蘇てしまいました。

以前と違って、使用者がスパムか否かを判断することに時間がかかり、

多様な語彙が含まれて人間か否か判断ができなくなりますた。

今回の逆襲はかなり鋭かったようで、Googleも対策を作り出すのに数か月の時間がかかりました。

その数か月の間「最近Googleはいかれた」「Googleはもうダメだ」との反応で、

Googleのイメージは急に落ち込みました。






<h2>２０２４年、遅かったが威力的な応手</h2>

結果から言うとGoogleは強力な対策を構え、汚名返上を果たしました。その秘訣は何でしょうか。

Googleは今回の攻撃を防ぐ為にもっと複雑なアルゴリズムを導入しました。

代表的には：

- 独自のAIコンテンツ探知アルゴリズムを使用：AI生成による文書構造を探知：論理構造が完璧、感情無し、など
- 既存アルゴリズムの強化：Experience, Expertise, Authoritativeness, Trustworthinessの評価を追加
1. Experience：実際に存在する著者名が載っているか
2. Expertise：専門的なドメイン・サイトなのか
3. Authoritativeness：使用者経験を基づいたか
4. Trustworthiness：出所やレビューのように信頼できるか
- 機械的な生成を探知：１つのドメインで短時間に数百個の投稿がアプロードされたか？

この策を行った後、Googleの上位にAI生成ブログ顕著に減少し、戦場をひっくり返しました。

しかし、

平均的なブロガーがアプロードした投稿の方がChatGPTの論理構成・データ量・簡潔性の面でましなはずですが、

AI生成物だから接近ができなくなることが、本当に最善でしょうか。






今のスパムブロガーはGoogleのアルゴリズムを欺くため、AIに人間らしさを要求すべきになりました。

わざわざプロンプトにいくつの欠点を入れて人間のようなミスも再現しています。

考えてみれば、多くの人に読みやすく、数えられない量のデータの組み合わせの、AI生成物は実は使用者とって

ブロガーの情報より適切な情報を得ることができる手段かもしれません。

しかし、もしこのままAIの作成物がどんどん増え続ける、「死んだネット理論」が現実になる可能性も考えるべきです。

もしこのテーマに興味があれば、ぜに「SEO」と「死んだネット理論」について調べましょう！